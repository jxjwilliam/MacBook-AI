## ðŸ“– GGUF

- is a model format used by Hugging Face for storing and sharing pre-trained models and datasets. HF models: GGUF
- GGUF/llama.cpp
- a quantization method that allows users to use the CPU to run an LLM but also offload some of its layers to the GPU for a speed up.


- `brew install llama.cpp`
- `git clone https://github.com/ggerganov/llama.cpp`, cmake, make, 
- download `model weights` separately and place in its `models` folder
- `llama.cpp`: Is main goal is to provide efficient LLM inference with minimal setup and high performance across various hardware platforms, including CPUs and GPUs.


### ðŸ¥ƒ
