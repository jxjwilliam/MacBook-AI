### â“ Where do we get all training data?

- The internet e.g. web pages, Wikipedia, forums, books, scientific articles, code bases, etc.
- Public datasets
    * Common Crawl (Colossal Clean Crawled Corpus i.e. C4, Falcon RefinedWeb)
    * The Pile
    * Hugging Face Datasets
- Private data sources e.g. FinPile (BloombergGPT) drawn from Bloomberg archives
- Use an LLM e.g. Alpaca - an LLM trained on structured text generated by GPT-3
- COCO

### â“ Alpaca

- JSON format
- instruction, input, output

### ğŸ’ semantic segmentation è¯­ä¹‰åˆ†å‰²

- Stable Diffusion uses it: U-NET

### â“ Quantized model vs. Full model

### ğŸ’

### â“

### ğŸ’ Training Images Overview

We maintain a public repository within the companyâ€™s internal shared drive, where each data scientist has a dedicated folder. Within these folders, there are recursively organized sub-folders that contain images in various formats. These images are utilized by data scientists for training purposes using YOLO, CUDA, and GPU technologies.
To efficiently archive and process these images, we have established a data pipeline that operates periodically to:

1. Transfer images from individual data scientist folders to Amazon MinIO artifacts.
2. Utilize Python scripts to download these images from the artifacts into a Kubernetes/Docker environment for training.
3. Monitor training results through a WebUI interface.

### â“ Training Images Management and Processing

Our company maintains a public repository on the internal shared drive, organized with unique folder names for each data scientist. These folders contain recursive sub-folders that store images in various formats. These images are used for training models like YOLO, leveraging CUDA and GPU resources.

To streamline the archiving and processing of these images, we have implemented a data pipeline that periodically performs the following tasks:

1. Image Archival: Moves images from the data scientists' individual folders to Amazon MinIO for centralized artifact storage.
2. Data Retrieval: Python scripts download these images from MinIO artifacts into the Kubernetes/Docker training environment.
3. Training Monitoring: Results from the training process are visualized and monitored through a WebUI.

Objective

Design an efficient workflow for data management, ensuring scalability, reliability, and seamless integration into the training pipeline. The focus is on leveraging the most reasonable solutions to optimize the training journey.

### ğŸ’ Object Detection

### â“

### ğŸ’

### â“

### ğŸ’

### â“

### ğŸ’

### â“

### ğŸ’

### â“

### ğŸ’

### â“

### ğŸ’

### â“

### ğŸ’
