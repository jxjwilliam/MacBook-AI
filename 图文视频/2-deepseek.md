## Deepseek R1

uses chain of thought reasoning, reinforcement learning, and expert architectures to achieve top-tier performance efficiently. ğŸš€

- Reasoning Model
- thinking
- Chain of Thoughts (CoT)
- **Reinforcement Learning** (RL): reward
- **Supervised FineTuning** (SFT)
- Distilled models: Llama, Qwen
- model translation: from MoE(R1-zero) to tranditional transfomer (Llama).
- MoE
- Multi-head latent Attention
- R1 built on top of R1-Zero

### RLHF

Reinforcement Learning from Human Feedback (RLHF) is a machine learning technique that trains an AI agent using direct human feedback to optimize its performance

## V3

`git clone https://huggingface.co/deepseek-ai/DeepSeek-V3-Base`

## ğŸ¥ƒ è’¸é¦ vs é‡åŒ–

- æ¸…æ™°çš„çŸ¥è¯†ç»“æ„å›¾è°±
- MoE: 37B/æ¯ä¸ªæ¨¡å—ï¼Œ20åˆ†ä¹‹ä¸€ã€‚
- MHLA: Multi Head Latent Attentionï¼ˆå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼‰

æ¨¡å‹å‹ç¼©çš„4å¤§æ–¹æ³•ï¼š

- è’¸é¦
- é‡åŒ–
- å‰ªæ
- äºŒå€¼åŒ–

## DeepSeek GRPO

- Group Relative Policy Optimization
- Unsloth


## DeepSeek SFT

- hugginface, modelscope download deepseek.
- deepseek-r1 7bï¼Œ24GB VRAM
- deepseek-r1-distill-qwen-7b
- å…¬å¸é‡Œä¸éœ€è¦é‡åŒ– ï¼ˆ4090ï¼‰
- qwen-7b-Instruct æ¨¡å‹
- deepseek è’¸é¦ qwen-7bçš„æ¨¡å‹

## LatentSync
